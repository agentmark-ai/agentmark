---
title: Run Inference
description: Execute prompts and get responses from language models
---

The `runInference` function is the primary way to execute prompts and get responses from language models in AgentMark.

## Basic Usage

```typescript TypeScript
import { runInference, load } from "@puzzlet/agentmark";

// Load prompt from file
const Prompt = await load('./example.prompt.mdx');

// Run inference with props
const result = await runInference(Prompt, {
  name: "Alice",
  items: ["apple", "banana"]
});
```

## API Reference

### runInference(prompt, props?, options?)

Executes a prompt with the given props and returns the model's response.

#### Parameters

| Parameter | Type | Description | Required |
|-----------|------|-------------|-----------|
| `prompt` | `AgentMarkPrompt` | The prompt to execute (loaded from an .mdx file) | Yes |
| `props` | `Record<string, any>` | Props to pass to the prompt | No |
| `options` | `InferenceOptions` | Optional configuration | No |

#### InferenceOptions

| Property | Type | Description | Required |
|----------|------|-------------|-----------|
| `apiKey` | `string` | Override the API key for the request | No |
| `telemetry` | `TelemetrySettings` | Telemetry data configuration | No |

#### TelemetrySettings

| Property | Type | Description | Required |
|----------|------|-------------|-----------|
| `isEnabled` | `boolean` | Whether telemetry is enabled | No |
| `functionId` | `string` | Identifier for the function | No |
| `metadata` | `Record<string, any>` | Additional metadata | No |

#### Returns: AgentMarkOutput

| Property | Type | Description |
|----------|------|-------------|
| `result.text` | `string` | Text response from the model |
| `result.object` | `Record<string, any>` | Structured output if schema is used |
| `tools` | `Array<{name: string, input: Record<string, any>, output?: Record<string, any>}>` | Tool calls made during inference |
| `toolResponses` | `GenerateTextResult<any, never>['toolResults']` | Results from tool executions |
| `usage.promptTokens` | `number` | Number of tokens in the prompt |
| `usage.completionTokens` | `number` | Number of tokens in the completion |
| `usage.totalTokens` | `number` | Total tokens used |
| `finishReason` | `"stop" \| "length" \| "content-filter" \| "tool-calls" \| "error" \| "other" \| "unknown"` | Why the model stopped generating |

## Examples

### Basic Text Response

```typescript TypeScript
const response = await runInference(Prompt, {
  userName: "Alice"
});
console.log(response.result.text); // "Hello Alice!"
```

### Structured Output

> Note: To use structured output, you must define a `schema` in the prompt.

```typescript TypeScript
const response = await runInference(Prompt, {
  sentence: "Alice and Bob went to lunch"
});
console.log(response.result.object); // { names: ["Alice", "Bob"] }
```

### With Custom Settings

```typescript TypeScript
await runInference(Prompt, props, {
  apiKey: "sk-1234567890",
  telemetry: {
    isEnabled: true,
    functionId: "example-function",
    metadata: {
      userId: "123"
    }
  }
});
```

## Error Handling

The function throws errors for:
- Invalid prompt format
- Model API errors
- Schema validation failures

```typescript TypeScript
try {
  const response = await runInference(Prompt);
} catch (error) {
  console.error("Inference failed:", error);
}
```

## Best Practices

1. Always handle potential errors
2. Use appropriate types for props
3. Configure telemetry appropriately for production use
4. Keep API keys secure and use environment variables