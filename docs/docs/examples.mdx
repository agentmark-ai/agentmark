---
title: Examples
---

# Examples

Using PromptDX is easy to get started with. Feel free to copy the below examples, try your own inputs, switch the models, etc.
It's recommended to download our [vscode extension](https://marketplace.visualstudio.com/items?itemName=puzzlet.promptdx) for rapid development.

## Basic

`index.prompt.mdx`
```jsx
---
name: basic-prompt
metadata:
  model:
    name: gpt-4o-mini
    settings:
      top_p: 1
      temperature: 0.7
---

<System>You are an expert math tutor</System>
<User>What's 2 + 2?</User>
```

Running via SDK:
```tsx
import { runInference, load } from '@puzzlet/promptdx';

const props = {};
const Prompt = await load('./index.prompt.mdx');
const result = await runInference(Prompt, props);
```

## Chatbot (w/ ChatHistory Component)

`index.prompt.mdx`
```jsx
---
name: chat
metadata:
  model:
    name: gpt-4o-mini
    settings:
      top_p: 1
      temperature: 0.7
test_settings:
  props:
    messageHistory:
      - role: user
        message: What's 2 + 2?
      - role: assistant
        message: 5
---

import ChatHistory from './chat-history.mdx';

<System>You are an expert math tutor</System>
<ChatHistory history={props.history}>
<User>That's wrong. What's 2 + 2?</User>
```

`chat-history.mdx`
```jsx
<ForEach arr={props.history}>
  {(item) => (
    <>
      <If condition={item.role == 'user'}>
        <User>{item.message}</User>
      </If>
      <ElseIf condition={item.role == 'assistant'}>
        <Assistant>{item.message}</Assistant>
      </ElseIf>
    </>
  )}
</ForEach>
```

Running via SDK:
```tsx
import { runInference, load } from '@puzzlet/promptdx';

const props = {
  history: [
    {
      role: "user",
      message: "What's 2 + 2?"
    },
    {
      role: "assistant",
      message: "5"
    },
  ]
};
const Prompt = await load('./index.prompt.mdx');
const result = await runInference(Prompt, props);
```

## Tools

Optionally return a data format which adheres to tools schema.

`index.prompt.mdx`
```jsx
---
name: tools
metadata:
  model:
    name: gpt-4o-mini
    settings:
      temperature: 0.7
      top_p: 1
      stream: true
      output: text
      tools:
        weather:
          description: Fetches the current weather for a specified location.
          parameters:
            type: object
            properties:
              name:
                type: string
                description: location
---

<System>You are a helpful assistant able to access the weather.</System>
<User>What is the current weather in Cleveland?</User>
```

Running via SDK:
```tsx
import Prompt from './index.prompt.mdx';
import { runInference, load } from '@puzzlet/promptdx';

const props = {};
const Prompt = await load('./index.prompt.mdx');
const result = await runInference(Prompt, props);
```

## Object Schema

Return an object that adheres to a given schema.

`index.prompt.mdx`
```jsx
---
name: schema
metadata:
  model:
    name: gpt-4o-mini
    settings:
      temperature: 0.7
      top_p: 1
      stream: true
      output: object
      schema:
        type: object
        properties:
          names:
            type: array
            items:
              type: string
            description: names of people
        required:
        - names
---

<System>You are a helpful assistant capable of finding all the names of the people in a given body of text.</System>
<User>Jessica and Michael decided to host a barbecue at their house, inviting their closest friends, Emily, David, and Sarah. As the evening went on, Jessica shared stories from her recent trip, while Michael grilled burgers, and Emily entertained everyone with her hilarious anecdotes.</User>
```

Running via SDK:
```tsx
import Prompt from './index.prompt.mdx';
import { runInference, load } from '@puzzlet/promptdx';

const props = {};
const Prompt = await load('./index.prompt.mdx');
const result = await runInference(Prompt, props);
```

## Direct Provider Call

Grab the config, and call openai, anthropic, or any other provider directly instead of through `runInference`.

`index.prompt.mdx`
```jsx
---
name: direct-call
metadata:
  model:
    name: gpt-4o-mini
    settings:
      top_p: 1
      temperature: 0.7
---

<System>You are an expert math tutor</System>
<User>What's 2 + 2?</User>
```

Running via SDK:
```tsx
import { deserialize, load } from '@puzzlet/promptdx';
import { OpenAI } from 'openai';

const client = new OpenAI();

const props = {};
const Prompt = await load('./index.prompt.mdx');
const openAIConfig = await deserialize(Prompt, props);
const result = await client.chat.completions.create(openAIConfig);
```

## Custom Model

1. Register your model provider parser w/ "my-custom-model", by following steps outlined in "Supported Provders".

2. Invoke config
`index.prompt.mdx`
```jsx
---
name: custom
metadata:
  model:
    name: my-custom-model
    settings:
      top_p: 1
      temperature: 0.7
      custom_settings: 1
---

<System>You are an expert math tutor</System>
<User>What's 2 + 2?</User>
```

Running via SDK:
```tsx
import { runInference, load } from '@puzzlet/promptdx';

const props = {};
const Prompt = await load('./index.prompt.mdx');
const result = await runInference(Prompt, props);
```

## Chaining

Docs Coming soon.