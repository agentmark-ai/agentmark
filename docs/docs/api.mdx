---
title: API
---

# API

AgentMark supports 3 basic methods for interacting with prompts: `runInference`, `deserialize`, and `serialize`.

## runInference

### usage

```tsx
import { runInference, load } from '@puzzlet/agentmark';
...
const prompt = await load('./example.prompt.mdx');
const result = await runInference(prompt, props)
```

Run inference will take a prompt config, call the model provider's API, and return the result in AgentMark format.

### input

A valid AgentMark file. See [syntax](/syntax.mdx) for more.

### output

**AgentMarkOutput Properties**

| Property       | Type                                                                 | Description                                                                                                                                                                                                                 | Optional/Required |
|----------------|----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|
| `result`       | `object`                                                             | Contains the primary output of the prompt, which can be either text or an object.                                                                                                                                           | Required          |
| ├─ `text`      | `string`                                                             | The generated text result.                                                                                                                                                                                                  | Optional          |
| └─ `object`    | `Record<string, any>`                                                | The generated object result.                                                                                                                                                                                                | Optional          |
| `tools`        | `Array<{ name: string; input: Record<string, any>; output?: Record<string, any>; }>` | An array detailing the tools invoked during the prompt execution. Each tool includes:  - `name`: The tool's name.  - `input`: The input parameters provided to the tool.  - `output` (optional): The tool's output. | Optional          |
| `usage`        | `object`                                                             | Provides token usage statistics for the prompt execution.                                                                                                                                                                   | Required          |
| ├─ `promptTokens`     | `number`                                                             | The number of tokens used in the prompt.                                                                                                                                                                                     | Required          |
| ├─ `completionTokens` | `number`                                                             | The number of tokens generated in the completion.                                                                                                                                                                           | Required          |
| └─ `totalTokens`      | `number`                                                             | The total number of tokens consumed (sum of prompt and completion tokens).                                                                                                                                                   | Required          |
| `finishReason` | `"stop" \| "length" \| "content-filter" \| "tool-calls" \| "error" \| "other" \| "unknown"` | Indicates the reason why the prompt generation was terminated. Possible values include:  - `"stop"`: The generation was successfully completed.  - `"length"`: The generation stopped due to reaching the maximum token length.  - `"content-filter"`: The content was filtered due to policy restrictions.  - `"tool-calls"`: The generation involved tool calls.  - `"error"`: An error occurred during generation.  - `"other"`: Terminated for reasons not specified above.  - `"unknown"`: The termination reason is unknown. | Required          |

### example:

example.prompt.mdx:
```jsx
---
name: my-prompt
metadata:
  model:
    name: gpt-4o-mini
---

<User>
Say: {props.greeting}
</User>
```

Run:
```tsx
import { runInference, load } from '@puzzlet/agentmark';
...
const props = { greeting: 'Hello' };
const prompt = await load('./example.prompt.mdx');
await runInference(myPrompt, props);
```

Result:
```tsx
Hello
```

## deserialize

Deserialize takes a prompt in AgentMark format, and turns it into the configuration needed to call a model provider.

### usage

```tsx
import { deserialize } from '@puzzlet/agentmark';
...
await deserialize(prompt, props)
```

### input

A valid AgentMark file. See [syntax](/syntax.mdx) for more.

### output

A valid consumer (OpenAI, Anthropic, etc.) format for running inference w/ the given config.

### example

my-prompt.mdx:
```jsx
---
name: my-prompt
metadata:
  model:
    name: gpt-4o-mini
---

<User>
Say: {props.greeting}
</User>
```

Run:
```tsx
const props = { greeting: 'Hello' };
await deserialize(myPrompt, props);
```

Result:
```tsx
{
  "model": "gpt-4o-mini",
  "messages": [
    {
      "content": "Say: Hello",
      "role": "user",
    }
  ]
}
```

## serialize

Serialize allows you to take an existing configuration for a Prompt, and serialize it into a AgentMark file.

### usage

```tsx
import { serialize } from '@puzzlet/agentmark';
...
serialize(config, modelName, promptName)
```


### input

A valid consumer (OpenAI, Anthropic, etc.) format for running inference w/ the given config.

### output

A valid AgentMark file. See [syntax](/syntax.mdx) for more.

### example

Input Config:
```json
{
  "model": "gpt-4o-mini",
  "top_p": 1,
  "temperature": 0.7,
  "messages": [
    {
      "content": "Say: {props.greeting}",
      "role": "user",
    }
  ]
}
```

Run:
```tsx
serialize(config, "gpt-4o-mini", "basic-prompt")
```

Result (serialized string):
```jsx
---
name: my-prompt
metadata:
  model:
    name: gpt-4o-mini
    settings:
      top_p: 1
      temperature: 0.7
---

<User>
Say: {props.greeting}
</User>
```